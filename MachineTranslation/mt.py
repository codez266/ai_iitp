import numpy as np
import pdb
from scipy.sparse.sparsetools import csr_scale_rows
from scipy.sparse import csr_matrix
from scipy.sparse import find
from sklearn import preprocessing
import math
import argparse
TRAIN = 0.8
class MT:
    def __init__(self, l1, l2, prefix):
        self.l1 = l1
        self.l2 = l2
        self.prefix = prefix

    def gen_vocab(self, vocab_file):
        vocab = {}
        with open(vocab_file) as fin:
            for line in fin:
                d = line.strip().split()
                vocab[d[1]] = int(d[0])
        vocab['^^'] = 0
        vocab['^^^'] = len(vocab)
        return vocab

    def emission(self, prob_file):
        probs = np.loadtxt(self.prefix + prob_file, dtype=np.float32)
        self.em_rows = probs[:,0]
        self.em_cols = probs[:,1]
        self.em_data = probs[:,2]
        table = csr_matrix((self.em_data, (self.em_rows, self.em_cols)), shape=(len(self.l1v), len(self.l2v)))
        return table.log1p()
    
    def transition(self, trg):
        d2 = self.l2v
        N = len(self.l2v)
        rows, cols, data = [], [], []
        with open(trg) as fin:
            for line in fin:
                l = line.strip().split()
                l = ['^^'] + l + ['^^^']
                for idx, w in enumerate(l):
                    if idx == 0:
                        continue
                    rows.append(d2[w])
                    cols.append(d2[l[idx-1]])
                    data.append(1)
        table = csr_matrix((data, (rows, cols)), shape=(N,N))
        table = preprocessing.normalize(table, norm = 'l1')
        return table.log1p()

    def search(self, sentence):
        emission = self.emission
        transition = self.transition
        N = transition.shape[0]
        d1, d2 = self.l1v, self.l2v
        print(len(d1),len(d2))
        vector = [d1[w] for w in sentence]
        line = [d1['^^']] + vector + [d1['^^^']]
        seqscore = np.zeros((len(d2), len(line)), dtype=np.float32)
        backptr = np.zeros((len(d2), len(line)), dtype=np.int32)
        seqscore[0][0] = 1
        backptr[0][0] = d1['^^']
        em_rows, em_cols, em_data = self.em_rows, self.em_cols, self.em_data
        for t, w in enumerate(line):
            wordindex = w
            #em_row = emission.getrow(wordindex).toarray()
            cols = em_cols[em_rows == wordindex]
            if t == 0:
                continue
            for k1, v1 in d2.items():
                maxScore = 0
                # current word index
                idx1 = v1
                if not str(idx1).isdigit():
                    continue
                #locs = np.where(nonzero_r == idx1)
                #indices = nonzero_c[locs]
                #vals = nonzero_data[locs]
                tr_row = transition.getrow(idx1)
                rs, indices, vals = find(tr_row)
                for idx2, v2 in zip(indices, vals):
                    e_prob = 0
                    val = em_data[cols == idx1]
                    if val.size == 0:
                        continue
                    e_prob = val[0]
                    score = seqscore[idx2][t-1] + v2 + e_prob
                    if score > maxScore:
                        #print(score)
                        maxScore = score
                        seqscore[idx1][t] = maxScore
                        backptr[idx1][t] = idx2
        i, score = 0, 0
        l2out = [0] * len(line)
        for j in range(len(d2)):
            if score < seqscore[j][-1]:
                score, i = seqscore[j][-1], j
        l2out[len(line)-1] = i
        for j in range(len(line)-2,-1,-1):
            l2out[j] = backptr[i][j+1]
            i = l2out[j]
        inv_d = {v: k for k, v in d2.items()}
        return [inv_d[w] for w in l2out]

    def train(self):
        self.l1v = self.gen_vocab(self.l1+'.vcb')
        self.l2v = self.gen_vocab(self.l2+'.vcb')
        self.emission = self.emission("t3.final")
        self.transition = self.transition(self.l2)

    def translate(self, sentences, outfile): 
        translated = []
        for line in sentences:
            words = sentences.strip().split()
            res = self.search(sent)
            sent = []
            for w in res:
                sent.append(w)
            translated.append(sent)
        return translated
    
    def readSentences(self, filename):
        sent = []
        with open(filename) as fname:
            for line in fname:
                l = line.strip()
                sent.append(l)
        return sent

def main():
    parser = argparse.ArgumentParser(description='Machine Translation using HMM')
    parser.add_argument('src',  help='source language')
    parser.add_argument('dest',  help='destiation language')
    parser.add_argument('prefix',  help='prefix of the files generated by GIZA++')
    parser.add_argument('-train', action='store_true',  help='Whether to train model or not')
    parser.add_argument('-test',  help='test file')

    args = parser.parse_args()
    src_file, dest_file, prefix = '', '', ''
    if args.src:
        src_file = args.src
    if args.dest:
        dest_file = args.dest
    if args.prefix:
        prefix = args.prefix
    print(prefix)
    mt_ob = MT(src_file, dest_file, prefix)
    if args.train:
        mt_ob.train()
    if args.test:
        sent = mt_ob.readSentences(args.test)
        trans = mt_ob.translate(sent)
        print(trans)
main()
